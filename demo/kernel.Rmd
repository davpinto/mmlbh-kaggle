---
title: "Validating some Beliefs about Income Bias"
author: "David Pinto"
date: "March 31, 2017"
output: 
   github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, 
                      warning = FALSE, fig.align = "center")
```

## Introduction

In this kernel we will use `tidyverse` to validate some popular hypothesis about bias in income. Then we will fit some machine learning models to get more accurate answers.

Some analyses presented here were took from the book [Mastering Python for Data Science](https://www.amazon.com/Mastering-Python-Science-Samir-Madhavan/dp/1784390151).

So, let's get started!

## Load Required Packages

We will need the following `R` packages:

```{r}
library(readr)    # To import data
library(magrittr) # To use the pipe %>% operator
library(tidyr)    # To clean and format data
library(dplyr)    # To manipulate data
library(purrr)    # To do functional programming
library(ggplot2)  # To visualize data
library(caret)    # To do machine learning
library(fastknn)  # To fit a fast KNN model
library(ranger)   # To fit a fast Random Forest model
```

Here we will use the notation `package_name::function_name()` to make it easy to know the package that provides each function.

## Data Preprocessing

### Import Data

```{r}
adult <- readr::read_csv("adult.csv.zip", na = "?")
dplyr::glimpse(adult)
```

Now, let's check the class proportions:

```{r}
table(adult$income)
```

So, we have an **imbalanced classification** dataset.

### Select Variables

Let's remove meaningless variables:

```{r}
dataset <- dplyr::select(adult, -fnlwgt, -education.num,
                         -capital.gain, -capital.loss)
```

### Transform Variables

Now, we will transform all `character` columns to `factor`, a more efficient format to represent categorical variables.

```{r}
chr.vars <- which(purrr::map_lgl(dataset, is.character))
dataset  <- dplyr::mutate_each(dataset, funs(as.factor), chr.vars)

## Compare object sizes
sprintf(
   "character size: %s; factor size: %s",
   format(object.size(adult$native.country), units = "KB"),
   format(object.size(dataset$native.country), units = "KB")
)
```

### Deal with Missing Values

The `adult` dataset has some missing values. More specifically:

```{r}
# Number of instances with missing values
sum(!complete.cases(adult))
```

The simplest way to impute missing values consists in replacing them with the `mean` or `median` for numerical variables and with the `mode` for categorical ones, as follows:

```{r}
na.fill <- purrr::map(dataset, function(column) {
   if (is.numeric(column)) {
      return(median(column))
   } else {
      column.tbl <- table(column)
      return(names(column.tbl)[which.max(column.tbl)])
   }
})
dataset <- tidyr::replace_na(dataset, replace = na.fill)
dplyr::glimpse(dataset)
```

Or we can just remove instances/observations with missing values:

```{r}
dataset <- na.omit(dataset)
```

## Exploratory Data Analysis

Let's explore the dataset and understand the patterns with the data before building any machine learning model.

### Hypothesis 1: People who are older earn more

The variable `age` is numeric, so we can check its distribution with respect to the `income` labels using a density plot:

```{r}
ggplot(dataset, aes(x = age, color = income, fill = income)) +
   geom_density(alpha = 0.8) +
   labs(x = "Age", y = "Density", title = "People who are older earn more",
        subtitle = "Density plot")
```

Or a boxplot:

```{r}
ggplot(dataset, aes(x = income, y = age, fill = income)) +
   geom_boxplot(alpha = 0.6, outlier.shape = NA) +
   labs(x = "Income", y = "Age", title = "People who are older earn more",
        subtitle = "Box and whisker plot")
```

Now let's estimate the central tendency for the `age` in both groups:

```{r}
dataset %>% 
   dplyr::group_by(income) %>% 
   dplyr::summarise(age = median(age))
```

So, people who earn above 50K tend to be aged around 44, while people who earn below 50K tend to be aged around 34.

### Hypothesis 2: Income bias based on working class

We can use a stacked bar chart to visualize the income class proportions for each working class, as follows:

```{r}
ggplot(dataset, aes(x = workclass, fill = income, color = income)) +
   geom_bar(alpha = 0.8, position = "fill") +
   coord_flip() +
   labs(x = "Working Class", y = "Proportion", title = "Income bias based on working class",
        subtitle = "Stacked bar plot")
```

Now, to be more precise, let's calculate the class proportions:

```{r}
prop.table(table(dataset$workclass, dataset$income), 1) * 100
```

We see that people who are self-employed and have a company are more likely to earn above 50K.

### Hypothesis 3: People with more education earn more

Education is an important field. It should be related to the level of earning power of an individual:

```{r}
ggplot(dataset, aes(x = education, fill = income, color = income)) +
   geom_bar(alpha = 0.8, position = "fill") +
   coord_flip() +
   labs(x = "Education Level", y = "Proportion", title = "People with more education earn more",
        subtitle = "Stacked bar plot")
```

```{r}
prop.table(table(dataset$education, dataset$income), 1) * 100
```

We can see a strong income bias: `Doctorate` > `Masters` > `Bacharelors`. So, the more the person is educated, the greater the probability to earn above 50K.

### Hypothesis 4: Married people tend to earn more

Let's do the same thing considering now the marital status:

```{r}
ggplot(dataset, aes(x = marital.status, fill = income, color = income)) +
   geom_bar(alpha = 0.8, position = "fill") +
   coord_flip() +
   labs(x = "Marital Status", y = "Proportion", title = "Married people tend to earn more",
        subtitle = "Stacked bar plot")
```

```{r}
prop.table(table(dataset$marital.status, dataset$income), 1) * 100
```

Wee can see that people who are married earn better as compared to people who are single.

### Hypothesis 5: There is a bias in income based on race

Let's see how earning power is based on the race of the person:

```{r}
ggplot(dataset, aes(x = race, fill = income, color = income)) +
   geom_bar(alpha = 0.8, position = "fill") +
   coord_flip() +
   labs(x = "Race", y = "Proportion", title = "There is a bias in income based on race",
        subtitle = "Stacked bar plot")
```

```{r}
prop.table(table(dataset$race, dataset$income), 1) * 100
```

Asian Pacific people and Whites have the highest earning power.

### Hypothesis 6: There is a bias in the income based on occupation

Let's see how earning power is based on the occupation of the person:

```{r}
ggplot(dataset, aes(x = occupation, fill = income, color = income)) +
   geom_bar(alpha = 0.8, position = "fill") +
   coord_flip() +
   labs(x = "Occupation", y = "Proportion", title = "There is a bias in the income based on occupation",
        subtitle = "Stacked bar plot")
```

```{r}
prop.table(table(dataset$occupation, dataset$income), 1) * 100
```

We can see that people who are in specialized or managerial positions earn more.

### Hypothesis 7: Men earn more

Let's see how earning power is based on gender

```{r}
ggplot(dataset, aes(x = sex, fill = income, color = income)) +
   geom_bar(alpha = 0.8, width = 0.5, position = "fill") +
   coord_flip() +
   labs(x = "Gender", y = "Proportion", title = "Men earn more",
        subtitle = "Stacked bar plot")
```

```{r}
prop.table(table(dataset$occupation, dataset$income), 1) * 100
```

That's true! It seems that males have a higher earning power as compared to females.

### Hypothesis 8: People who work in more hours earn more

Let's see how earning power is related to people working hours per week:

```{r}
ggplot(dataset, aes(x = income, y = hours.per.week, fill = income)) +
   geom_boxplot(alpha = 0.6, outlier.shape = NA) +
   labs(x = "Income", y = "Hours per Week", title = "People who work in more hours earn more",
        subtitle = "Box and whisker plot")
```

```{r}
dataset %>% 
   dplyr::group_by(income) %>% 
   dplyr::summarise(hours = mean(hours.per.week))
```

We can see that people who earn above 50K work more than 40 hours in average.

### Hypothesis 9: There is a bias in income based on the country of origin

Finally, let's see how earning power is related to a person's country of origin:

```{r}
ggplot(dataset, aes(x = native.country, fill = income, color = income)) +
   geom_bar(alpha = 0.8, width = 0.8, position = "fill") +
   coord_flip() +
   labs(x = "Native Country", y = "Proportion", title = "There is a bias in income based on the country of origin",
        subtitle = "Stacked bar plot")
```

```{r}
prop.table(table(dataset$native.country, dataset$income), 1) * 100
```

We can see that Taiwanese, French, Iranians, and Indians are the most well-earning people among different countries.

## Classification using Machine Learning Models

How can we combine all these variables to predict the income level of an unseen person?

The answer is: Machine Learning!

### Decision Tree

- Decision trees can deal with categorical variables directly.

- They don't require data scaling. 

```{r}
# Split data: 70% for training
set.seed(1024)
tr.idx <- caret::createDataPartition(dataset$income, p = 0.7, list = FALSE)

# Fit a decision tree
tree.model <- caret::train(form = income ~ ., data = dataset[tr.idx,], 
                           method = "rpart")

# Predict test set
tree.pred <- predict(tree.model, newdata = dataset[-tr.idx,])

# Performance
classAccuracy <- function(y, yhat) {
   sum(yhat == y) / length(y)
}
classAccuracy(dataset$income[-tr.idx], tree.pred)
```

### K Nearest Neighbor (KNN)

- Non-linear learner.

- Simple to understand and implement.

```{r}
# Encode categorical features
x <- model.matrix(income ~ . -1, data = dataset)
y <- dataset$income
dim(x)
```

```{r}
# Fit KNN using the 'dist' method
knn.model <- fastknn(x[tr.idx,], y[tr.idx], x[-tr.idx,], k = 25)
classAccuracy(dataset$income[-tr.idx], knn.model$class)
```

```{r}
# Fit KNN using the 'vote' method
knn.model <- fastknn(x[tr.idx,], y[tr.idx], x[-tr.idx,], k = 25, 
                     method = "vote")
classAccuracy(dataset$income[-tr.idx], knn.model$class)
```

```{r}
# Fit KNN using the 'dist' method with normalization
knn.model <- fastknn(x[tr.idx,], y[tr.idx], x[-tr.idx,], k = 25, 
                     method = "vote", normalize = "robust")
classAccuracy(dataset$income[-tr.idx], knn.model$class)
```

```{r, results='hide'}
# Find the best k
set.seed(2048)
cv.out <- fastknnCV(x[tr.idx,], y[tr.idx], k = seq(from = 10, to = 30, by = 5), 
                    method = "vote", normalize = "robust", 
                    folds = 10, nthread = 8)
cv.out$cv_table
```

```{r, echo=FALSE}
knitr::kable(cv.out$cv_table, digits = 3)
```

### Random Forest

- Combine simple models (decision trees) to build a more complex one.

- The base models can be trained in parallel.

- Allow ranking the variables.

```{r}
# Fit Random Forest
rf.model <- ranger::ranger(income ~ ., dataset[tr.idx,], num.trees = 500, 
                           mtry = 2, importance = "impurity", num.threads = 8)

# Predict test set
rf.pred <- predict(rf.model, dataset[-tr.idx,])

# Performance
classAccuracy(dataset$income[-tr.idx], rf.pred$predictions)
```

Computing variable importance:

```{r, results='hide'}
# Variable importance
var.imp <- ranger::importance(rf.model)
var.imp <- dplyr::data_frame(variable = names(var.imp), importance = var.imp) %>% 
   dplyr::arrange(desc(importance))
var.imp
```

```{r, echo=FALSE}
knitr::kable(var.imp, digits = 3)
```
