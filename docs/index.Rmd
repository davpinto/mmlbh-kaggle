---
title: "Introdução ao Kaggle"
subtitle: "[David Pinto](http://davpinto.com), CAO Nexer Labs"
author: "5º Meetup de ML"
date: "Belo Horizonte - 31/03/2017"
output:
  xaringan::moon_reader:
    lib_dir: libs
    yolo: false
    nature:
      highlightStyle: github
      highlightLines: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, cache = TRUE)
```

name: about-me
class: top, left
background-image: none

# Quem sou eu?

- Bacharel em **Engenharia de Controle e Automação** pela *UFMG*

- Mestre em **Inteligência Computacional** pelo *PPGEE* da *UFMG*

- **Co-founder** e **CAO** da Nexer

---
template: about-me

.footnote[
------

- **Website:** [http://davpinto.com](http://davpinto.com)
- **GitHub:** [https://github.com/davpinto](https://github.com/davpinto)
- **Email:** david@nexer.com.br
]

???
(Quem vos fala?) Falar sobre minha formação, com foco em como iniciei na área de machine learning e como comecei a trabalhar de fato na área. Meu primeiro contato foi já na graudação. Contar a história de como iniciei na Nexer (Danilo Mattos, CEO, chegou no meu laboratório da pós-graduação dizendo que estava coletando dados de carros e precisava analisá-los. Foi quando identificamos alguns padrões que indicavam que o veículo tinha 5 marchas. No entanto, o algoritmo acusava 7. Era um modelo que estava pra entrar no mercado, e realmente tinha 7 marchas.)

---

# O que já vimos?

- Roadmap de formação do Cientista de Dados: [Como Iniciar em Machine Learning?](http://davpinto.com/ml-presentation)

- Introdução ao R: [Data Science usando R](https://github.com/davpinto/mmlbh-eda)

- Análise Explortória de Dados: [Análise de Dados na prática usando R](https://davpinto.github.io/mmlbh-eda)

???
Machine learning é uma ferramenta do cientista de dados. Corresponde a uma das áreas do conhecimento que auxilia no trabalho do cientista de dados. Antes de aplicar as técnicas de ML, o cientista de dados passa certa de 80% do seu tempo manipulando os dados. Então fizemos uma demonstração da análise exploratória de dados, que é um processo de entendimento dos dados no qual dataviz exerce um papel muito importante.

---
class: top, left
background-image: url(./img/cisco-roadmap.png)
background-size: 70%
background-position: center middle

# Como se tornar um Cientista de Dados?

.footnote[
------

Link: [Becoming a Data Scientist](http://blog.kaggle.com/2017/03/02/becoming-a-data-scientist-profiling-ciscos-data-science-certification-program/).
]

???
Este é um ótimo roadmap indicado no blog do Kaggle. Foi lançado um pouco depois da minha palestra. Então fica a dica agora. Vocês vão ver que machine learning é um dos módulos desse guia de formação.

---
count: false
class: inverse, center, top
background-image: url(./img/practice.png)
background-size: 60%
background-position: bottom right

# The one thing you need to
# Master Data Science:

---
class: top, left

# Prática Deliberada

É a prática que realmente te coloca em um novo nível de habilidade. Isso se consegue quando você se esforça para fazer algo que ainda não consegue fazer bem. Para isso você precisa: 

- De um sistema projetado para te desafiar

- Repetir a prática até adquirir um skill natural

- De feedbacks de pessoas mais experientes para se tornar mais efetivo

- Ter consciência de que não é fácil, pois exige muito esforço

- Ter consciência de não é divertido, pois pode ser frustrante às vezes

.footnote[
------

- Artigo original: [The one thing you need to master data science](http://sharpsightlabs.com/blog/one-thing-master-data-science/)

- Livro: [Talent is overrated](https://www.amazon.com.br/Talent-Overrated-Separates-World-Class-Performers/dp/1591842948)
]

???
O artigo derruba o mito de talento inato e defende a prática como o único meio de elevar as skills de data science a um novo patamar. No entanto, é muito comum praticarmos uma habilidade sem nos larçarmos a novos desafios. Geralmente quando praticamos, focamos em coisas que já sabemos fazer. Por exemplo, se você tem habilidade em plotar gráficos em barra, você tenderá a usar essa habilidade sempre que quiser representar graficamente os dados. Dessa forma você estará praticando, mas não estará adquirindo novas skills. Estará somente intensificando uma skill que você já tem. 

---
count: false
class: inverse
background-image: url(./img/kaggle-logo.png)
background-size: 50%
background-position: center middle

---
class: top, left
background-image: url(./img/kaggle-home.png)
background-size: 60%
background-position: center middle

# O que é e como funciona?

.footnote[
------

[www.kaggle.com](https://www.kaggle.com)
]

???
Você pode participar de competições e analisar bases de dados diversas, compartilhar análises e participar de discussões a respeito das suas soluções e dos demais membros da comunidade. Na plataforma você será desafiado, poderá repetir as práticas, receberá feedbacks e poderá discutir suas soluções. Ao mesmo tempo, o leaderboard dos desafios é uma forma de feedback muito boa. A plataforma utiliza prêmios em dinheiro e técnicas de gamificação para tornar a prática muito mais instigante e divertida.

---
class: top, left
background-image: url(./img/kaggle-logo.png)
background-size: 20%
background-position: 90% 90%

# O que é e como funciona?

[Kaggle](www.kaggle.com) é a maior comunidade de cientistas de dados do mundo, os quais competem através de uma plataforma *online* com o objetivo de solucionar problemas complexos de *Data Science*. As competições são geralmente vinculadas a grandes empresas, que fornecem dados de seus processos e desafiam os competidores a solucionarem problemas associados ao seu *business*, premiando as melhores soluções.

---
background-image: url(./img/kaggle-profile.png)
background-size: 60%
background-position: center middle

# Meu Perfil

---
class: center
background-image: url(./img/otto-group.png)
background-size: 70%
background-position: 50% 76%

# Minha Primeira Competição
![](./img/my-ranking.png)

???
Na época eu estava no mestrado. Estava buscando novas bases de dados para testar meus algoritmos. Rodei vários modelos que eu conhecia, e não entendia porque minha performance não ficava próxima às dos melhores colocados. A partir de então passei a estudar mais soluções vencedoras e a ler os fórums de algumas competições. Aprendi muito! Mas que no meu mestrado e mais que nos livros.

---
class: top, left
background-image: url(./img/otto-winner.png)
background-size: 80%
background-position: center 60%

# Solução Vencedora

Link: [Otto Group Winner](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov)

???
Foi vencida por um brasileiro. Comentar sobre a combinação de modelos e a geração de novas features. Analisar alguns comentários dos demais competidores.

---

# O que mais aprendi?

- Geração de novas *features* usando o modelo KNN. Na tentativa de reproduzir a solução, acabei obtendo *insights* para criar a biblioteca [fastknn](https://davpinto.github.io/fastknn/), que hoje está disponível na plataforma.

- Combinação de modelos usando *Stacking*. Demorei um tempo pra entender como funcionava de fato. Aprendi tentando reproduzir e criei um tutorial que mostra todos os passos: [Stacking usando H2O.ai](https://davpinto.github.io/h2o-tutorial). O código está disponível no meu [GitHub](https://github.com/davpinto/h2o-tutorial).

---

# Dicas para subir no ranking

### Feature Engineering

- Tratamento de variáveis esparsas (muitos zeros)

- Tratamento de variáveis nominais com muitas categorias

- Remoção de variáveis com baixa variabilidade

- Remoção de variáveis com multicolinearidade

- Tratamento de *missing values* (Decision Tree Imputation, KNN, MICE)

- Transformação de variáveis (log, sqrt, Box-Cox, binning)

- Seleção de características (Regularização L1, Ensemble Trees, Boruta)

- Extração de caracteríticas (MDS, PCA, GLRM, t-SNE, Autoencoder)

---

# Dicas para subir no ranking

### Ajuste de Parâmetros (*Hyperparameter Optimization*)

- Grid Search

- Random Search (Bergstra and Bengio 2012)

- Bayes Optimization

---

# Dicas para subir no ranking

### Combinação de Modelos

- Bagging and Boosting

- Voting e Weighted Voting

- Blending (Average e Weighted Average)

- Stacking (geração de *metafeatures* para combinação de modelos)

---

# Bibliotecas mais utilizadas e wrappers para R

- **Sofia-ML** (C/C++): pacote `RSofia`. SVM linear. Projeto Google.

- **Vowpal Wabbit** (VW) (C/C++): pacote `RVowpalWabbit`. Algoritmos de classificação e regressão altamente eficientes. Projeto Microsoft/Yahoo!.

- **H2O.ai** (Java): pacote `h2o`. Algortimos diversos: Naive-Bayes, GLM, RF, GBM, DL e GLRM.

- **ExtraTrees** (Java): pacote `extraTrees`. Comitês de árvores de decisão.

- **XGBoost** (C/C++): pacote `xgboost`. Gradient Boosting Machines com modelos de regressão linear e árvores de decisão.

- **MXNet** (C/C++): pacote `mxnet`. Todos os modelos de Deep Learning (redes feedforward, convolucionais e recorrentes)

---
count: false
class: inverse, center
background-image: url(./img/kaggle-datasets.png)
background-size: 75%
background-position: center middle

# Kaggle Datasets

---

# Kaggle Datasets

No ano passado o site foi reestruturado e passou a rankear os competidores não só pelo desempenho nas competições, mas também pontuando participações nos fóruns e compartilhamento de scripts. A pontuação dos dois últimos quesitos é dada pelo número de *upvotes* que uma resposta ou script recebe dos demais competidores.

Você pode compartilhar *scripts* (*kernels*) dentro de uma competição ou para um **Kaggle Dataset**. Qualquer usuário do site pode criar um *dataset*, e todos os datasets ficam disponíveis para os demais. Uma ótima forma de treinar tanto as técnicas de *machine learning* como seus skills de comunicação é criando *kernels* para diferentes *datasets*.

---
background-image: url(./img/adult-dataset.png)
background-size: 75%
background-position: center middle

# Vamos praticar?

.footnote[
------

Link: [Adult dataset](https://www.kaggle.com/uciml/adult-census-income)
]

---

# Sugestões para próximos *hands on*

- Discutir um dataset por evento abordando uma teoria diferente de ML

- Entender as técnicas de combinação de modelos

- Usando gráficos animados para entender algoritmos de ML

- Fazer demonstrações com as bibliotecas mais famosas de ML

- Criação de uma competição no Kaggle entre os membros do Meetup

### Vote no Slack do Meetup

---
count: false
class: inverse, center, middle

# Perguntas?
